rd_("lGet N valueslGet Z valuesAjContains the success value0000000000000AaProbability typesBkInitializes a norm scaler that uses l1 normBkInitializes a norm scaler that uses l2 normClR squared coefficient, is the proportion of the variance \xe2\x80\xa6000AhContains the error value0000000000000AcFittable algorithmsBnThe log link function <code>g(x)=log(x)</code>BiFitted Principal Component Analysis modelBdFitted Support Vector Machines modelCmPerforms the matrix product between the kernel matrix and \xe2\x80\xa6CcSet the machine-precision regularization in the \xe2\x80\xa6DeSet the precision parameter (distortion, <code>eps</code>) of the \xe2\x80\xa6AfSet stopping conditionAbStopping conditionCoGiven an input matrix <code>observations</code>, with shape \xe2\x80\xa6DhFit an elastic net model given a feature matrix <code>x</code> and a \xe2\x80\xa6DkFit a multi-task Elastic Net model given a feature matrix <code>x</code>\xe2\x80\xa6mFit the modelDhFit an LARS model given a feature matrix <code>x</code> and a target \xe2\x80\xa6DjFit an isotonic regression model given a feature matrix <code>X</code> \xe2\x80\xa6DkFit a linear regression model given a feature matrix <code>X</code> and \xe2\x80\xa6DhGiven a 2-dimensional feature matrix array <code>x</code> with shape \xe2\x80\xa60DjLearns a vocabulary from the documents in <code>x</code>, according to \xe2\x80\xa60CnFits the input dataset accordng to the scaler method. Will \xe2\x80\xa6DjLearns a vocabulary from the texts in <code>x</code>, according to the \xe2\x80\xa6DhFit a decision tree using <code>hyperparamters</code> on the dataset \xe2\x80\xa6BlInitializes a norm scaler that uses max normB`Matthew Correlation CoefficientsCjCreate a wrapper model from a list of single-target modelsBiCreates probability from the given float.BmCreate a new dataset from records and targetsCkCreate new GaussianNbParams set with default values for \xe2\x80\xa6CnCreate new MultinomialNbParams set with default values for \xe2\x80\xa6ClCreate new BernoulliNbParams set with default values for \xe2\x80\xa6De<code>new</code> lets us configure our training algorithm parameters:BkCreate default elastic net hyper parametersCkCreate a new AdaBoost parameter set with default values \xe2\x80\xa6CbCreate new hyperparameters with pre-defined valuesCgCreate a new model with given parameters, number of \xe2\x80\xa6ClCreate new FastICA algorithm with default values for its \xe2\x80\xa6BdCreate default Lars hyper parametersBkCreate a default isotonic regression model.ClCreate a default linear regression model. By default, an \xe2\x80\xa6DcCreates a <code>BallTreeIndex</code> using the K-D construction \xe2\x80\xa6BlCreates an instance of <code>BallTree</code>BfCreates a new <code>KdTreeIndex</code>BjCreates an instance of <code>KdTree</code>BlCreates a new <code>LinearSearchIndex</code>C`Creates an instance of <code>LinearSearch</code>CaInitializes the scaler with the specified method.BeCreates the set of default parametersAjCreate hyper parameter setAiInitialize a solver stateCkCreates a new Tikz structure for the decision tree with \xe2\x80\xa6CjCreates an instance of a Whitener that uses the PCA methodAlReturns the random generatorAgRandom number generatorChSet random number generator. Used to initialize Z valuesCfSums all elements in the same row of the kernel matrixBhSet tolerance on upate at each iterationBgStopping criterion for the LBFGS solverCgCreates an instance of a Whitener that uses the ZCA \xe2\x80\xa6Cdeach component has its own general covariance matrixCkMeasures the degree of probability of a randomly chosen \xe2\x80\xa6AjLink functions used by GLMClStruct to print a fitted decision tree in Tex using tikz \xe2\x80\xa6C`Assigns the Ward dissimilarity between clusters.AgSet the beta parameter.AfGet the beta parameterBoEstimated coefficients for the linear predictorBbPerforms K-folding on the dataset.AoReturns the argument unchanged.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000BaCalls <code>U::from(self)</code>.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000CbRead in the iris-flower dataset from dataset path.CcReturns an iterator over the samples in the datasetDhSetter for <code>kind</code>, whether to construct a dense or sparse \xe2\x80\xa6CiThe link function of the GLM, for mapping from linear \xe2\x80\xa6BoCompute the link function <code>g(ypred)</code>ClThe means that will be subtracted to the features before \xe2\x80\xa6oReturn the meanCfGives the size of the side of the square kernel matrixAbSwap two variablesAkCreates a view of a datasetCmGives a KernelView which has a view on the original kernel\xe2\x80\xa60ChError type resulting from failed hyperparameter checkingAfFloating point numbersBbSome standard non-linear functionsCmSpecifies the methods an inner matrix of a kernel must be \xe2\x80\xa6oDiscrete labelsCbThe logit link function <code>g(x)=logit(x)</code>AbFitted Platt modelBkError when performing Max operation on dataAjGet the variance smoothingClSpecifies the portion of the largest variance of all the \xe2\x80\xa610AfSet the learning rate.AgGet the alpha parameterCkConstant that multiplies with the penalty term and thus \xe2\x80\xa6DhSpecial case of <code>blobs_with_distribution</code> with a standard \xe2\x80\xa6AeReturn the k-th boundCnSet the C value for regression and solver epsilon stopping \xe2\x80\xa6CkSet the C value and optionnaly an epsilon value used in \xe2\x80\xa6CfChecks the hyperparameters and returns the checked \xe2\x80\xa6CbReturns the depth of the node in the decision treeAdError types in LinfaBcError definitions for preprocessingCnG function used in the approximation to neg-entropy, refer \xe2\x80\xa6BcIndex of the sample in the dataset.AiCreate a LASSO only modelBdCreate a multi-task Lasso only modelDg<code>linfa</code> aims to provide a comprehensive toolkit to build \xe2\x80\xa6CgThe power determines the underlying target distributionAiCreate a ridge only modelBdCreate a multi-task ridge only modelB`Set whether to scale the dataset00AoSet the Hessian\xe2\x80\x99s sigma valueClReturn the split (feature index, value) and its impurity \xe2\x80\xa6CaSet the number of steps in the diffusion operatorBoErrors encountered when using argmin\xe2\x80\x99s solverClDBSCAN (Density-based Spatial Clustering of Applications \xe2\x80\xa6CnGMM fitting algorithm is initalized with the result of the \xe2\x80\xa6CkK-means clustering aims to partition a set of unlabeled \xe2\x80\xa6gKD TreeCnImplementation of K-D tree, a fast space-partitioning data \xe2\x80\xa6CdType definition of Kernel that owns its inner matrixAhL1 or Manhattan distanceAhL2 or Euclidean distanceAmGet the labels in all targetsAgEuclidean inner productAiL-p or Minkowsky distanceCbAssigns the median dissimilarity between clusters.CfA method for computing the dissimilarities between \xe2\x80\xa6CmOPTICS (Ordering Points To Identify Clustering Structure) \xe2\x80\xa6BnGMM fitting algorithm is initialized randomly.B`Pick random points as centroids.DoSimplified <code>Result</code> using <code>NaiveBayesError</code> as error typeDoSimplified <code>Result</code> using <code>ElasticNetError</code> as error typeDiSimplified <code>Result</code> using <code>FtrlError</code> as error typeEaSimplified <code>Result</code> using <code>HierarchicalError</code> as error typeDiSimplified <code>Result</code> using <code>TSneError</code> as error typeCmThis struct represents a data point in the dataset with it\xe2\x80\xa6CjAssigns the minimum dissimilarity between all pairs of \xe2\x80\xa6DkComputes the idf as <code>log(1+n/1+document_frequency) + 1</code>. The \xe2\x80\xa6CmA sparse kernel requires to define a number of neighbours \xe2\x80\xa6CbIndices of active variables at the end of the pathCfMaximum of covariances (in absolute value) at each \xe2\x80\xa6BhGetter for a column of the kernel matrixCkGet the model positive and negative classes mapped to theirDdSetter for <code>method</code>, the inner product used by the kernelCaThe inner product that will be used by the kernel00AlSetter for the scaler methodCmReturns the method used for fitting. Useful for printing, \xe2\x80\xa6CmReturns the inverse document frequency method used in the \xe2\x80\xa6AnThe fitted base learner modelsCkThe number of iterations taken by the algorithm to find \xe2\x80\xa6CjSet the number of initializations to perform. The best \xe2\x80\xa6CgThe final results will be the best output of n_runs \xe2\x80\xa6BgChange the value of <code>n_runs</code>B`Return number of total variablesCfSet the Nu value for regression and solver epsilon \xe2\x80\xa6ChSet the Nu and optionally a C value (default 1.) for \xe2\x80\xa6CfCreate default parameter set for the Platt scaling \xe2\x80\xa6BfConstruct a new set of hyperparameters00CmConfigures the hyperparameters with the minimum number of \xe2\x80\xa60CfCreate a default parameter set for construction of \xe2\x80\xa6CmCreate a default parameter set for construction of Follow \xe2\x80\xa6CjGenerates the default set of parameters for building a \xe2\x80\xa6BdCreate default Lars hyper parametersAiGet the fitted parametersBaConstruct a new set of parametersAlCreate default parameter setDjCreate new parameters for a <code>RandomProjection</code> with default \xe2\x80\xa6ClDefaults are provided if the optional parameters are not \xe2\x80\xa6CnRecall score, the number of correct classifications in the \xe2\x80\xa6DkArray of size <code>n_features</code> that contains the scale that will \xe2\x80\xa6BlReturn target as positive/negative indicatorCbProvide traits for different classes of algorithmsCiUpdate method of the model hyperparameters in case of \xe2\x80\xa6BgApply whitening to the embedding vectorCjAssigns the average dissimilarity between all pairs of \xe2\x80\xa6AkThe checked hyperparametersgDatasetCnMeasures the entropy of a subtree, defined as the sum over \xe2\x80\xa6BoFitted FastICA model for recovering the sourcesAfIncremental algorithmsCmError returned when performing spatial queries on nearest \xe2\x80\xa6AbPredict with modellRecord traitBlThe unique class labels seen during trainingCjGet the list of class labels, which maps the numerical \xe2\x80\xa6hDatasetsBnDistance metric used in the DBSCAN calculationAgSet the distance metricAkReturns the distance metricBlDistance metric to be used for the algorithm0BoReturn the eigenvalue of the diffusion operatorlF-beta-scoreCkReturn the sum of distances between each training point \xe2\x80\xa6DcComputes the inverse link function <code>h(linear predictor)</code>BhReturns true if the node has no childrenAkInitializes a MaxAbs scalerClSet the maximum number of iterations in the optimization \xe2\x80\xa6CjCommon metrics functions for classification and regressionCiInitializes a MinMax scaler with range <code>0..=1</code>BkSet the minimum stepsize in the line searchCgGet the number of steps taken in optimization algorithm0BaReturn number of active variablesCbNearest neighbour algorithm used for range queriesBnSet the nearest neighbour algorithm to be usedCjNearest Neighbour algorithm to use to find the nearest \xe2\x80\xa60DdSetter for <code>nn_algo</code>, nearest neighbour algorithm for \xe2\x80\xa6DgArray of size <code>n_features</code> that contains the offset that \xe2\x80\xa6CnSet the overall parameter penalty parameter of the elastic \xe2\x80\xa6nLinfa prelude.AkReturn records of a datasetCcProduces a shuffled version of the current Dataset.AlReturns reference to targetsAeSet output verbosity.AjReturns optionally weightsBhReturns the model weights (alpha values)CaSingular vectors of the cross-covariance matrices00AeCalculate the Z score0BfA fitted AdaBoost ensemble classifier.iBall TreeCjImplementation of ball tree, a space partitioning data \xe2\x80\xa6CdAssigns the centroid dissimilarity between clusters.CjAssigns the maximum dissimilarity between all pairs of \xe2\x80\xa6CnA distance function that can be used in spatial algorithms \xe2\x80\xa6BgGaussian(eps): exp(-norm(x - x\xe2\x80\x99)/eps)BfAn error when modeling a GMM algorithmBnThe identity link function <code>g(x)=x</code>AoL-infinte or Chebyshev distanceCfLevel-order (BFT) iterator of nodes in a decision treeBkTextbook definition of idf, computed as \xe2\x80\xa6AkA node in the decision treeCdAssigns the weighted dissimilarity between clusters.CmStruct that can be fitted to the input data to obtain the \xe2\x80\xa6nAccuracy scoreCfExtracts a slice containing all samples in the datasetAnGet the binarization thresholdAnSet the binarization thresholdBlReturns both children, first left then rightClCreates an instance of a Whitener that uses the cholesky \xe2\x80\xa6CcWhether a complete Tex document should be generatedBnRead in the diabetes dataset from dataset pathCiGetter for the elements in the diagonal of the kernel \xe2\x80\xa6CnComputes the distance between two points. For most spatial \xe2\x80\xa6BmF1-score, this is the F-beta-score for beta=1CgFast algorithm for Independent Component Analysis (ICA)BfReturn features_idx of this tree (BFT)BjCalibrate another model with Platt scalingClPerforms a single batch update of the Mini-Batch K-means \xe2\x80\xa6CbUtility functions for randomly generating datasetsCkSet l1_ratio parameter of the elastic net. Controls how \xe2\x80\xa6CbSet l1_ratio parameter. Controls how the parameterAoGet the L1 regularization valueCnSet l2_ratio parameter. Controls how the parameter penalty \xe2\x80\xa6AoGet the L2 regularization valueDi<code>linfa-nn</code> provides Rust implementations of common spatial \xe2\x80\xa6ChRead in the physical exercise dataset from dataset path.AoLoadings of records and targets00CbLog loss of the probabilities of the binary targetBkSet maximum number of iterations during fitCaMaximum number of iterations for the LBFGS solverBdSet the maximal number of iterationsCcNumber of vocabulary entries learned during fitting0BeReturns the number of support vectorsBkReturn the number of targets in the datasetAmInitializes a Standard scalerCnSpecify the random number generator to use to generate the \xe2\x80\xa6CjReturn a reference to single or multiple target variables.AnCriterion when to stop mergingiDatasetPrClThe set of hyperparameters that can be specified for the \xe2\x80\xa6DiComputes the idf as <code>log(n/document_frequency) +1</code>. The \xe2\x80\x9c\xe2\x80\xa6BfPincipal Component Analysis parametersCmError variants from hyper-parameter construction or model \xe2\x80\xa6CiSet the algorithm used to estimate the first singular \xe2\x80\xa600BlApply bootstrapping for samples and featuresCnReturn the set of centroids as a 2-dimensional matrix with \xe2\x80\xa6CmChecks the hyperparameters and returns a reference to the \xe2\x80\xa6AmValidate the hyper parameters00AdValidates parametersCjGet the varying values of the coefficients along the path.AbComposition modelsAdReturn the embeddingCkLearns a vocabulary from the documents contained in the \xe2\x80\xa60BoReturns the true-positive, false-positive curveCgGet the fitted intercept, 0. if no intercept was fittedCkGet the fitted intercept, [0., \xe2\x80\xa6, 0.] if no intercept \xe2\x80\xa611BkIntercept or bias added to the linear modelBeWhether the kernel is a linear kernelCkPerforms k-folding cross validation on fittable algorithms.DjReturns the <code>k</code> points in the index that are the closest to \xe2\x80\xa6BdIndependent Component Analysis (ICA)AePartial Least SquaresAgSupport Vector MachinesAlReturn max depth of the treeCiSets the optional limit to the depth of the decision treeBnMaximal error between two continuous variables000CnIf true, all charachters in the documents used for fitting \xe2\x80\xa60BcSet the Nu value for classificationCnPrecision score, the number of correct classifications for \xe2\x80\xa6CjA faster version of the distance metric that keeps the \xe2\x80\xa6AlReturn root node of the treeCiProjection matrices used to transform records and targets00AjShrink active variable setBlShould we shrink, e.g. ignore bounded alphasCeList of entries to be excluded from the generated \xe2\x80\xa60ChMaximum distance between two points to be considered \xe2\x80\xa6AaSet the toleranceCjSet the convergence threshold. EM iterations will stop \xe2\x80\xa6ChThe training is considered complete if the euclidean \xe2\x80\xa6BjChange the value of <code>tolerance</code>ChTwo points are considered neighbors if the euclidean \xe2\x80\xa6CeDistance between points for them to be considered \xe2\x80\xa6CmSet the tolerance which is the minimum absolute change in \xe2\x80\xa6CiSet the tolerance used as convergence criteria in the \xe2\x80\xa600CoGiven an input matrix <code>observations</code>, with shape \xe2\x80\xa6CfPerform hierarchical clustering of a similarity matrix0BnBuilds a kernel from a view of the input data.0CkBuilds a new Dataset with the kernel as the records and \xe2\x80\xa6100BnApply dimension reduction to the given dataset00DiGiven a sequence of <code>n</code> documents, produces a sparse array \xe2\x80\xa6ClSubstitutes the records of the dataset with their scaled \xe2\x80\xa6CnScales an array of size (nsamples, nfeatures) according to \xe2\x80\xa6CgScales all samples in the array of shape (nsamples, \xe2\x80\xa62DkGiven a sequence of <code>n</code> documents, produces an array of size \xe2\x80\xa6BhProject a kernel matrix to its embeddingBbCompute the embedding of a dataset0C`Compute the embedding of a two-dimensional array0AnMethods for uncorrelating dataCfError returned when building nearest neighbour indicesAaElastic Net modelAcReason for stoppingBgFitted Gaussian Naive Bayes classifier.CgSpecifies centroid initialization algorithm for KMeans.CmK-means|| algorithm, a parallelized version of K-means++. \xe2\x80\xa6A`A generic kernelCdKernel representation, can be either dense or sparseCgType definition of Kernel that borrows its inner matrixCmNorm scaler: scales all samples in a dataset to have unit \xe2\x80\xa6CgA set of hyperparameters whose values have not been \xe2\x80\xa6AnPlatt Newton\xe2\x80\x99s method errorsBbPolynomial(constant, degree):  \xe2\x80\xa6CgAdd the Lapack bound to the floating point of a datasetAeReturn the componentsCiBuilds a spatial index using a default leaf size. See \xe2\x80\xa6BoReturn the Pearson\xe2\x80\x99s Correlation CoefficientsAiGet the fitted hyperplane00BkCreate a node iterator in level-order (BFT)AmFollow the regularized leaderBaLeast angle regression a.k.a. LARet-SNEChThe number of clusters we will be looking for in the \xe2\x80\xa6BhReturn the number of leaves in this treeCbProduce N boolean targets from multi-class targetsBiSet the perplexity of the t-SNE algorithmEbReturns <code>Some(prediction)</code> for leaf nodes and <code>None</code> for \xe2\x80\xa6BmSet the dimension of output of the embedding.CnContains all vocabulary entries, in the same order used by \xe2\x80\xa6ClConstains all vocabulary entries, in the same order used \xe2\x80\xa6AfReturn a single weightBhFitted Bernoulli Naive Bayes classifier.kDatasetBasekDatasetViewAiWhen initial KMeans failsBiAn error when modeling a KMeans algorithmCiSpatial indexing structure created by <code>KdTree</code>ClAllows a kernel to have either a dense or a sparse inner \xe2\x80\xa6CcErrors encountered during linear algebra operations0BiAn error when modeling a Linear algorithmBhAn error when performing OPTICS AnalysisCmPrecomputed list of centroids, represented as an array of \xe2\x80\xa6AoCurrent state of the SMO solverCmMethods for computing the inverse document frequency of a \xe2\x80\xa6AiTransformation algorithmsBiCorrelation analysis for dataset featuresCfGet the duality gap at the end of the optimization \xe2\x80\xa60BfCalculate weights for model predictionClSet the method used to initialize the weights, the means \xe2\x80\xa6AoCluster initialization strategyBeChange the value of <code>init</code>kNaive BayesAfDecision tree learningBjMap targets with a function <code>f</code>CkSet the number of components to use, if not set all are \xe2\x80\xa6BhCreate self object from new target arrayCmConfigures the hyperparameters with the minimum number of \xe2\x80\xa60AiIterate over observationsAdIterate over targetsCaRead in the winequality dataset from dataset pathCnTransforms the input dataset by keeping only those samples \xe2\x80\xa6EgAdd trait bound <code>Lapack</code> and <code>Scalar</code> to NdArray\xe2\x80\x99s floating \xe2\x80\xa60BbAdd a legend to the generated treeAgSelect a merging methodCkReturn a mutable reference to single or multiple target \xe2\x80\xa6CjHelper struct for building a set of DBSCAN hyperparametersC`A fitted decision tree model for classification.BdEmbedding of diffusion map techniqueCgWhen a cluster has no more data point while fitting GMMBhAn error when modeling FastICA algorithmCnA specifier for the type of the relation between components\xe2\x80\xa6AnWhen inertia computation failsCgWhen any of the hyperparameters are set the wrong value00CeAn helper struct used to construct a set of valid \xe2\x80\xa6BnThe inner product definition used by a kernel.CfDefines the set of parameters needed to build a kernelCjThe result of fitting a linear scaler. Scales datasets \xe2\x80\xa6mLinear searchCjImplementation of linear search, which is the simplest \xe2\x80\xa6BkWhen fitting EM algorithm does not convergeCgWhen the distance between the old and new centroids \xe2\x80\xa6CkA fitted ensemble of Decision Trees trained on a random \xe2\x80\xa6B`Parameters of the solver routineClThe metric used to determine the feature by which a node \xe2\x80\xa6CaCalls <code>check()</code> and unwraps the resultCgThe coefficients of the linear model such that Y is \xe2\x80\xa600AeIterate over featuresClReturns the name of the feature used in the split if the \xe2\x80\xa6CfCalculate the Pearson Correlation Coefficients and \xe2\x80\xa6CbReturn the p values supporting the null-hypothesisnKernel methodsDj<code>linfa-linear</code> aims to provide pure Rust implementations of \xe2\x80\xa6CmGenerates a random Linfa::Dataset (ds). The ds values are \xe2\x80\xa6BoStop merging when a certain distance is reachedCgWhen building the vocabulary, only consider the top \xe2\x80\xa60AkThe base learner parametersBgThe model parameters for the base modelC`Returns the number of estimators in the ensembleCaSet the maximum number of weak learners to train.BiThe maximum number of estimators to trainDkIf set to <code>(1,1)</code> single tokens will be candidate vocabulary \xe2\x80\xa60AlSample normalization methodsCjStop merging when a certain number of clusters are reachedCiSet seed for random number generator for reproducible \xe2\x80\xa6AdReturns target namesDfSums the inner product of <code>sample</code> and every one of the \xe2\x80\xa6B`Updates the records of a datasetB`Updates the targets of a datasetB`Updates the weights of a datasetCkReturns all the points in the index that are within the \xe2\x80\xa6CkSpatial indexing structure created by <code>BallTree</code>CmA specifier for the method used for the initialization of \xe2\x80\xa6CgWhen any of the hyperparameters are set the wrong value0BjFitted Multinomial Naive Bayes classifier.BiPossible scaling methods for LinearScalerCjRemove the Lapack bound to the floating point of a datasetCjReturn the number of training points belonging to each \xe2\x80\xa6ChThe distance to the nth closest point where n is the \xe2\x80\xa6DfConverts the result of <code>distance</code> to <code>rdistance</code>CfSpecifies the number of models to fit in the ensemble.BdThe number of models in the ensembleAeReturns feature namesCfWhether to calculate the intercept for this model. \xe2\x80\xa6ClSpecifies whether a bias or intercept should be added to \xe2\x80\xa6BlSet the learning rate (shrinkage parameter).BgThe learning rate (shrinkage parameter)ClSets the model to use the Linear kernel. For this kernel \xe2\x80\xa6CnInitializes a MinMax scaler with the specified minimum and \xe2\x80\xa6CaThe weight (alpha) for each model in the ensembleClCreate a new AdaBoost parameter set with a fixed RNG for \xe2\x80\xa6CmCreates probability from the given float. Doesn\xe2\x80\x99t check \xe2\x80\xa6BoPredict a probability with the sigmoid functionBnImplement Platt calibration with Newton methodCnCompute probability estimates for each sample wrt classes. \xe2\x80\xa60CiReturns the responsibilities as a (n_obs, n_clusters) \xe2\x80\xa6DfConverts the result of <code>rdistance</code> to <code>distance</code>CnSet the probability threshold for which the \xe2\x80\x98positive\xe2\x80\x99 \xe2\x80\xa6CjSets the metric used to decide the feature on which to \xe2\x80\xa6AjGet the variance smoothingClSpecifies the portion of the largest variance of all the \xe2\x80\xa6CfA helper struct for building AdaBoost hyperparameters.BmReturn a reference to multi-target variables.BhTargets with precomputed, counted labelsChStruct that can be used to whiten data. Data will be \xe2\x80\xa6ClThe set of hyperparameters that can be specified for the \xe2\x80\xa6AeThe input is singularCnK-means++ algorithm. Using this over random initialization \xe2\x80\xa6CnThe analysis from running OPTICS on a dataset, this allows \xe2\x80\xa6CgPredict with model into a mutable reference of targets.AcSVM HyperparametersDgImplement this trait to opt into a blanket <code>Transformer</code> \xe2\x80\xa6AoConvert CSV bytes into 2D arrayCgCross validation for single and multi-target algorithmsClCreate targets that <code>predict_inplace</code> works with.CbCreate a t-SNE param set with given embedding sizeDkGenerates a <code>Tikz</code> structure to print the fitted tree in Tex \xe2\x80\xa6CmProduces a CountVectorizer with the input vocabulary. All \xe2\x80\xa60CkProduces a FittedTfIdfVectorizer with the input vocabulary.CaReturns the threshold corresponding to each pointAfLinear Scaling methodshDatasetsAlEnsemble Learning AlgorithmsAcLogistic RegressionCmSet the maximum number of iterations for the optimization \xe2\x80\xa6ClSet the maximum number of iterations of the power method \xe2\x80\xa600CnMinimum number of neighboring points a point needs to have \xe2\x80\xa6CiMinimum number of a points in a neighborhood around a \xe2\x80\xa6CiGenerate parameters with a specific nearest neighbour \xe2\x80\xa6ChNon-negative regularization added to the diagonal of \xe2\x80\xa6ChConfigure the elastic net model to fit an intercept. \xe2\x80\xa6CjConfigure the linear regression model to fit an intercept.EgAdd trait bound <code>Lapack</code> and <code>Scalar</code> to NdArray\xe2\x80\x99s floating \xe2\x80\xa60AmConvert to probability matrixBnReturn a reference to single-target variables.BkConfusion matrix for multi-label evaluationClCounts the occurrences of each vocabulary entry, learned \xe2\x80\xa6ClError variants from hyperparameter construction or model \xe2\x80\xa6CaA fitted ensemble of learners for classification.BhHelper trait to construct counted labelsBbWhen lower bound computation failsCfMerge models with binary to multi-class classificationCmError variants from hyper-parameter construction or model \xe2\x80\xa6CiEvaluates the quality of a clustering using euclidean \xe2\x80\xa6CcThe t-SNE algorithm is a statistical method for \xe2\x80\xa6DkSimlar to <code>CountVectorizer</code> but instead of just counting the \xe2\x80\xa6AnCalculate the confidence level0AhSet the covariance type.CnSets the model to use the Gaussian kernel. For this kernel \xe2\x80\xa6AeDimensional ReductionCnComputes the derivative of the link <code>g&#39;(ypred)</code>CjSets the minimum weight of samples that a split has to \xe2\x80\xa6BoSet the target number of non-zero coefficients.ClCreate default hyperparameters with custom random number \xe2\x80\xa6DjCreate new parameters for a <code>RandomProjection</code> with default \xe2\x80\xa6CbSet the C value for positive and negative samples.AjPredict something in placeChGiven one input observation, return the index of its \xe2\x80\xa6CoGiven an input matrix <code>observations</code>, with shape \xe2\x80\xa6EhGiven an input matrix <code>X</code>, with shape <code>(n_samples, n_features)</code>\xe2\x80\xa600AcRecover the sources1AbPredict the targetFeGiven an input matrix <code>X</code>, with shape <code>(n_samples, 1)</code>, <code>predict</code>\xe2\x80\xa63ClGiven a feature matrix, predict the classes learned when \xe2\x80\xa60444DeMake predictions for each row of a matrix of features <code>x</code>.AjReturn the singular valuesChInitializes a Stadard scaler that does not scale the \xe2\x80\xa6DjGiven a sequence of <code>n</code> file names, produces a sparse array \xe2\x80\xa6BeA hyper-parameter set for Elastic-NetClA hyper-parameter set during construction for a Gaussian \xe2\x80\xa6AkInvalid smoothing parameterCjAn ordinary least squares univariate linear regression \xe2\x80\xa6CfMerge models with single target to multi-target modelsCfNearest neighbour algorithm builds a spatial index \xe2\x80\xa6B`The input has not enough samplesCaWhen there are no samples in the provided datasetBfParameters for Platt\xe2\x80\x99s Newton methodAoEmbedding via random projectionCcIf we fail to compute any components of the SVD \xe2\x80\xa6CjGeneralized Linear Model (GLM) with a Tweedie distributionCkSet the approximation threshold of the Barnes Hut algorithmBcReturns the Area-Under-Curve metricCmCreate new hyperparameters with pre-defined random number \xe2\x80\xa6Dk<code>linfa-clustering</code> aims to provide pure Rust implementations \xe2\x80\xa6kElastic NetBkSet the number of EM iterations to perform.CiWe exit the training loop when the number of training \xe2\x80\xa6CaChange the value of <code>max_n_iterations</code>CjSets the minimum weight of samples required to split a \xe2\x80\xa6BhCreate self object from new target arrayCgSet the number of iterations after which the true P \xe2\x80\xa6BfEvaluates the quality of a clustering.CkSplit confusion matrix in N one-vs-all binary confusion \xe2\x80\xa6CiSplit confusion matrix in N*(N-1)/2 one-vs-one binary \xe2\x80\xa6BfSplit dataset into two disjoint chunks00CkInitializes a Standard scaler that does not subract the \xe2\x80\xa6CfReturns a mutable reference to multi-target variables.CmA hyper-parameter set during construction for a Bernoulli \xe2\x80\xa6ClThe set of hyperparameters that can be specified for the \xe2\x80\xa6BjError variants from parameter constructionCdAn error when fitting with an invalid hyperparameter2CoSpatial indexing structure created by <code>LinearSearch</code>3BiClassification for multi-label evaluationBgConvert Gzipped CSV bytes into 2D arrayAjApply sample bootstrappingCkEstimate the number of clusters in this embedding (very \xe2\x80\xa6CmConvert this linkage method into a nearest neighbor chain \xe2\x80\xa6CnTransform the given dataset in the projected space back to \xe2\x80\xa600BiTransform data back to its original spaceBkCalculates label frequencies from a datasetCiSets the model to use the Polynomial kernel. For this \xe2\x80\xa6CiCompute log-probability estimates for each sample wrt \xe2\x80\xa60AbRandom ProjectionsCkGetter for the data in the upper triangle of the kernel \xe2\x80\xa6C`Set the platt params for probability calibrationBeUpdates the target names of a datasetCgReturns a mutable reference to single-target variables.AmDiffusion map hyperparametersBiFast Independent Component Analysis (ICA)AmAn isotonic regression model.CnLinear Scaler: learns scaling parameters, according to the \xe2\x80\xa6BfA two-class logistic regression model.CbPearson Correlation Coefficients (or Bivariate \xe2\x80\xa6CgA helper struct for building a set of Random Forest \xe2\x80\xa6AkApply feature bootstrappingCiSpecifies the minimum and maximum (relative) document \xe2\x80\xa60BjSame as R-Squared but with biased variance000CcReturn the amount of explained variance per elementCmReturn the feature importance, i.e. the relative impurity \xe2\x80\xa6CgSets the proportion of the total number of training \xe2\x80\xa6CkThe proportion of the total number of training features \xe2\x80\xa6CkGenerates a three dimension swiss roll, centered at the \xe2\x80\xa6ChComputes the derivative of the inverse link function \xe2\x80\xa6AgHierarchical ClusteringBnReturn max and min gradients of free variablesCcMean squared error between two continuous variables000AjSelect optimal working setBfUpdates the feature names of a datasetBbSet the kernel to use for trainingCeThe set of valid hyperparameters for the AdaBoost \xe2\x80\xa6BeAgglomerative hierarchical clusteringAkMultiTask Elastic Net modelCcA hyper-parameter set during construction for a \xe2\x80\xa6mPreprocessingBkMean error between two continuous variables000CiCalculate the Pearson Correlation Coefficients from a \xe2\x80\xa6DnRun Newton\xe2\x80\x99s method to find optimal <code>A</code> and <code>B</code> valuesB`Classification for binary-labelsCmGaussian Mixture Model (GMM) aims at clustering a dataset \xe2\x80\xa6CgSets the proportion of the total number of training \xe2\x80\xa6CjThe proportion of the total number of training samples \xe2\x80\xa6CmIf true, all documents used for fitting will be converted \xe2\x80\xa60CmCompute the unnormalized posterior log probabilities. The \xe2\x80\xa6CmTerm frequency - inverse document frequency vectorization \xe2\x80\xa6CnA verified hyper-parameter set ready for the estimation of \xe2\x80\xa6CjA helper struct for building a set of Ensemble Learner \xe2\x80\xa6ClCounts the occurrences of each vocabulary entry, learned \xe2\x80\xa62BoRegression metrices trait for multiple targets.CnA spatial index structure over a set of points, created by \xe2\x80\xa6DhSpecialized version of <code>cross_validate</code> for single-target \xe2\x80\xa6CfMedian absolute error between two continuous variables000ClSets the minimum decrease in impurity that a split needs \xe2\x80\xa6ClGiven a feature matrix, predict the probabilities that a \xe2\x80\xa6CnReturn normalized probabilities for each output class. The \xe2\x80\xa6CiThe reachability distance of a sample is the distance \xe2\x80\xa6AjSelect optimal working setBdThe matrix used for scaling the data<C`Enum that dispatches to one of the crate\xe2\x80\x99s \xe2\x80\xa6CjA fitted linear regression model which can be used for \xe2\x80\xa6BaRandom projection hyperparametersBmRegression metrices trait for single targets.ClThe set of hyperparameters that can be specified for the \xe2\x80\xa6BlApply bootstrapping for samples and featuresCbReturn the mean impurity decrease for each featureCgMean squared log error between two continuous variables000CiComputes a similarity matrix with gaussian kernel and \xe2\x80\xa6ChThe set of hyperparameters that can be specified for \xe2\x80\xa6AmDiffusion map hyperparametersBnA multinomial class logistic regression model.CdValidated version of <code>LogisticRegression</code>DaGiven an input matrix <code>blob_centroids</code>, with shape \xe2\x80\xa6ClA fitted isotonic regression model which can be used for \xe2\x80\xa6CgA fitted logistic regression which can make predictionsAjInvalid stopping conditionCnA verified hyper-parameter set ready for the estimation of \xe2\x80\xa6CcChecked version of <code>HierarchicalCluster</code>CjReturn the normalized amount of explained variance per \xe2\x80\xa6C`A hyper-parameter set for multi-task Elastic-NetClBuilds a spatial index using a MxN two-dimensional array \xe2\x80\xa6ClCount vectorizer: learns a vocabulary from a sequence of \xe2\x80\xa6CkThe set of valid hyper-parameters that can be specified \xe2\x80\xa6CjCreate a t-SNE param set with given embedding size and \xe2\x80\xa6CfReturn the relative impurity decrease for each featureBaRandom projection hyperparametersClThe set of hyperparameters that can be specified for the \xe2\x80\xa6CmCalculates label frequencies from a dataset while masking \xe2\x80\xa6CiValidated version of <code>MultiLogisticRegression</code>CkA fitted multinomial logistic regression which can make \xe2\x80\xa6=AjApply sample bootstrappingCiMean absolute percentage error between two continuous \xe2\x80\xa6000ChA Receiver Operating Characteristic for binary-label \xe2\x80\xa6AkApply feature bootstrappingCfCalculate the Pearson Correlation Coefficients and \xe2\x80\xa6")