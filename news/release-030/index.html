<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="keywords" content="linfa,ml,stats,statistical learning, machine learning,rust">
  <meta name="description" content="comprehensive toolkit for statistical learning in rust">
  <title>Linfa Toolkit</title>

  <link rel="icon" href="mascot.svg">
  <link rel="stylesheet" href="https://rust-ml.github.io/linfa/style.css">
  <script src="https://rust-ml.github.io/linfa/code-examples.js"></script>
</head>

<body>
  <div class="top">
    <section class="grid header">
      <div class="logo">
        <img alt="logo" src="https://rust-ml.github.io/linfa/mascot.svg" height=60>
        <p>
          Linfa

          <span>Menu</span>
        </p>
      </div>
      <div class="header-main">
        <ul>
          <li><a href="https://rust-ml.github.io/linfa">Home</a></li>
          <li><a href="https://rust-ml.github.io/linfa/about">About</a></li>
          <li><a href="https://rust-ml.github.io/linfa/docs">Documentation</a></li>
          <li><a href="https://rust-ml.github.io/linfa/community">Community</a></li>
          <li><a href="https://rust-ml.github.io/linfa/news">News</a></li>
        </ul>
      </div>
    </section>
  </div>


  <section class="section">
    <div class="container">
      
<div class="grid"><div class="news-page">
<h1 class="title">
  Release 0.3.0
</h1>
<span>Published on January 21th, 2021</span>
<p>Linfa 0.3.0 concentrates on polishing the existing implementation and adds only three new algorithms to the crowd. A new feature system is introduced, which allows the selection of the BLAS/LAPACK backend in the base-crate. The <code>Dataset</code> interface is polished and follows the <code>ndarray</code> model more closely. The new <code>linfa-datasets</code> crate gives easier access to sample datasets and can be used for testing.</p>
<span id="continue-reading"></span><h1 id="new-algorithms">New algorithms</h1>
<ul>
<li>Approximated DBSCAN has been added to <code>linfa-clustering</code> by [@Sauro98]</li>
<li>Gaussian Naive Bayes  has been added to <code>linfa-bayes</code> by [@VasanthakumarV]</li>
<li>Elastic Net linear regression has been added to <code>linfa-elasticnet</code> by [@paulkoerbitz] and [@bytesnake]</li>
</ul>
<h1 id="changes">Changes</h1>
<ul>
<li>Added benchmark to gaussian mixture models (a3eede55)</li>
<li>Fixed bugs in linear decision trees, added generator for TiKZ trees (bfa5aebe7)</li>
<li>Implemented serde for all crates behind feature flag (4f0b63bb)</li>
<li>Implemented new backend features (7296c9ec4)</li>
<li>Introduced <code>linfa-datasets</code> for easier testing (3cec12b4f)</li>
<li>Rename <code>Dataset</code> to <code>DatasetBase</code> and introduce <code>Dataset</code> and <code>DatasetView</code> (21dd579cf)</li>
<li>Improve kernel tests and documentation (8e81a6d)</li>
</ul>
<h1 id="example">Example</h1>
<p>The following section shows a small example how datasets interact with the training and testing of a Linear Decision Tree.</p>
<p>You can load a dataset, shuffle it and then split it into training and validation sets:</p>
<pre class="giallo" style="color: #1F2328; background-color: #FFFFFF;"><code data-lang="rust"><span class="giallo-l"><span style="color: #6E7781;">// initialize pseudo random number generator with seed 42</span></span>
<span class="giallo-l"><span style="color: #CF222E;">let mut</span><span> rng</span><span style="color: #CF222E;"> =</span><span style="color: #953800;"> Isaac64Rng</span><span style="color: #CF222E;">::</span><span style="color: #8250DF;">seed_from_u64</span><span>(</span><span style="color: #0550AE;">42</span><span>);</span></span>
<span class="giallo-l"><span style="color: #6E7781;">// load the Iris dataset, shuffle and split with ratio 0.8</span></span>
<span class="giallo-l"><span style="color: #CF222E;">let</span><span> (train, test)</span><span style="color: #CF222E;"> =</span><span style="color: #953800;"> linfa_datasets</span><span style="color: #CF222E;">::</span><span style="color: #8250DF;">iris</span><span>()</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">shuffle</span><span>(</span><span style="color: #CF222E;">&amp;mut</span><span> rng)</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">split_with_ratio</span><span>(</span><span style="color: #0550AE;">0.8</span><span>);</span></span></code></pre>
<p>With the training dataset a linear decision tree model can be trained. Entropy is used as a metric for the optimal split here:</p>
<pre class="giallo" style="color: #1F2328; background-color: #FFFFFF;"><code data-lang="rust"><span class="giallo-l"><span style="color: #CF222E;">let</span><span> entropy_model</span><span style="color: #CF222E;"> =</span><span style="color: #953800;"> DecisionTree</span><span style="color: #CF222E;">::</span><span style="color: #8250DF;">params</span><span>()</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">split_quality</span><span>(</span><span style="color: #953800;">SplitQuality</span><span style="color: #CF222E;">::</span><span style="color: #953800;">Entropy</span><span>)</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">max_depth</span><span>(</span><span style="color: #953800;">Some</span><span>(</span><span style="color: #0550AE;">100</span><span>))</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">min_weight_split</span><span>(</span><span style="color: #0550AE;">10.0</span><span>)</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">min_weight_leaf</span><span>(</span><span style="color: #0550AE;">10.0</span><span>)</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">fit</span><span>(</span><span style="color: #CF222E;">&amp;</span><span>train);</span></span></code></pre>
<p>The validation dataset is now used to estimate the error. For this the true labels are predicted and then a confusion matrix gives clue about the type of error:</p>
<pre class="giallo" style="color: #1F2328; background-color: #FFFFFF;"><code data-lang="rust"><span class="giallo-l"><span style="color: #CF222E;">let</span><span> cm</span><span style="color: #CF222E;"> =</span><span> entropy_model</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">predict</span><span>(test</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">records</span><span>()</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">view</span><span>())</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">confusion_matrix</span><span>(</span><span style="color: #CF222E;">&amp;</span><span>test);</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span style="color: #8250DF;">println!</span><span>(</span><span style="color: #0A3069;">&quot;{:?}&quot;</span><span>, cm);</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span style="color: #8250DF;">println!</span><span>(</span></span>
<span class="giallo-l"><span style="color: #0A3069;">    &quot;Test accuracy with Entropy criterion: {:.2}%&quot;</span><span>,</span></span>
<span class="giallo-l"><span style="color: #0550AE;">    100.0</span><span style="color: #CF222E;"> *</span><span> cm</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">accuracy</span><span>()</span></span>
<span class="giallo-l"><span>);</span></span></code></pre>
<p>Finally you can analyze which features were used in the decision and export the whole tree it to a <code>TeX</code> file. It will contain a TiKZ tree with information on the splitting decision and impurity improvement:</p>
<pre class="giallo" style="color: #1F2328; background-color: #FFFFFF;"><code data-lang="rust"><span class="giallo-l"><span style="color: #CF222E;">let</span><span> feats</span><span style="color: #CF222E;"> =</span><span> entropy_model</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">features</span><span>();</span></span>
<span class="giallo-l"><span style="color: #8250DF;">println!</span><span>(</span><span style="color: #0A3069;">&quot;Features trained in this tree {:?}&quot;</span><span>, feats);</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span style="color: #CF222E;">let mut</span><span> tikz</span><span style="color: #CF222E;"> =</span><span style="color: #953800;"> File</span><span style="color: #CF222E;">::</span><span style="color: #8250DF;">create</span><span>(</span><span style="color: #0A3069;">&quot;decision_tree_example.tex&quot;</span><span>)</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">unwrap</span><span>();</span></span>
<span class="giallo-l"><span>tikz</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">write</span><span>(gini_model</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">export_to_tikz</span><span>()</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">to_string</span><span>()</span><span style="color: #CF222E;">.</span><span style="color: #8250DF;">as_bytes</span><span>())</span></span>
<span class="giallo-l"><span style="color: #CF222E;">    .</span><span style="color: #8250DF;">unwrap</span><span>();</span></span></code></pre>
<p>The whole example can be found in <a rel="external" href="https://github.com/rust-ml/linfa/blob/master/linfa-trees/examples/decision_tree.rs">linfa-trees/examples/decision_tree.rs</a>.</p>

</div></div>

    </div>
  </section>
  <div class="bottom">
    <div class="grid">
      <div class="footer">View
        <a href="https://github.com/rust-ml/linfa/tree/master/docs/website">page source</a>
        - any contribution is welcome!
      </div>
    </div>
  </div>
</body>

</html>